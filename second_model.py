# -*- coding: utf-8 -*-
"""Second Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2S5BWErGMCwTUMdjLAeJtoyKm9vtsDg

# Step 1: Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# import libraries"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate
from sklearn.metrics import classification_report

"""# Step 3: Load the Dataset

"""

file_path = '/content/drive/MyDrive/Islamabad_Crime_Records_2M.csv'
df = pd.read_csv(file_path)

print(df.head())

"""# Step 4: Feature Preprocessing

Text Features
"""

df['Combined_Text'] = df['Primary Type'] + " " + df['Description'] + " " + df['Location Description'] + " " + df['Block']

"""Tokenize and pad the text data."""

# Replace NaN values with an empty string
df['Combined_Text'] = df['Combined_Text'].fillna('')

# Convert all entries to strings
df['Combined_Text'] = df['Combined_Text'].astype(str)

# Now, tokenize and pad the sequences
tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(df['Combined_Text'])

sequences = tokenizer.texts_to_sequences(df['Combined_Text'])
padded_sequences = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')

"""Numeric Features

"""

scaler = MinMaxScaler()
numeric_features = ['Beat', 'District', 'Ward', 'Community Area', 'Year', 'X Coordinate', 'Y Coordinate']
df[numeric_features] = scaler.fit_transform(df[numeric_features])

"""Binary Features

# Step 5: Prepare Data for Training
"""

# Check for missing values in your features and target
print(df['Combined_Text'].isnull().sum())  # Check for NaNs in the text feature
print(df[numeric_features].isnull().sum())  # Check for NaNs in numeric features
print(df['Arrest'].isnull().sum())  # Check for NaNs in the target

# Fill missing values in the text column with an empty string
df['Combined_Text'] = df['Combined_Text'].fillna('')

# Fill missing values in numeric columns with the median
df[numeric_features] = df[numeric_features].fillna(df[numeric_features].median())

# Fill missing values in the target column
df['Arrest'] = df['Arrest'].fillna(df['Arrest'].mode()[0])

X_text = padded_sequences
X_numeric = df[numeric_features].values
y = df['Arrest'].values

# Check the shapes to ensure they match
print(X_text.shape)
print(X_numeric.shape)
print(y.shape)

from sklearn.model_selection import train_test_split

X_train_text, X_test_text, X_train_numeric, X_test_numeric, y_train, y_test = train_test_split(
    X_text, X_numeric, y, test_size=0.2, random_state=42)

"""# Step 6: Define the Model

"""

# Text input
text_input = Input(shape=(50,))
embedding = Embedding(input_dim=5000, output_dim=128, input_length=50)(text_input)
lstm = LSTM(64, return_sequences=False)(embedding)

# Numeric input
numeric_input = Input(shape=(len(numeric_features),))
dense_numeric = Dense(64, activation='relu')(numeric_input)

# Combine text and numeric inputs
concatenated = Concatenate()([lstm, dense_numeric])
dense = Dense(64, activation='relu')(concatenated)
dropout = Dropout(0.3)(dense)
output = Dense(1, activation='sigmoid')(dropout)

# Compile the model
model = Model(inputs=[text_input, numeric_input], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    [X_train_text, X_train_numeric], y_train,
    validation_data=([X_test_text, X_test_numeric], y_test),
    batch_size=128, epochs=10, verbose=1
)

"""# Evaluate the Model

"""

y_pred = (model.predict([X_test_text, X_test_numeric]) > 0.5).astype(int)
print(classification_report(y_test, y_pred))

"""# Step 9: Improve the Model

"""

# Importing necessary libraries
from keras.models import Model
from keras.layers import Embedding, LSTM, Dropout, Dense, Input, concatenate
from keras.regularizers import l2

# Text input model (Embedding + LSTM + Dropout)
text_input = Input(shape=(50,))  # Shape based on padded sequence length
embedding_layer = Embedding(input_dim=5000, output_dim=128)(text_input)  # Embedding layer
lstm_layer = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)  # LSTM layer
dropout_layer = Dropout(0.5)(lstm_layer)  # Dropout to prevent overfitting

# Numeric input model
numeric_input = Input(shape=(X_numeric.shape[1],))  # Shape based on number of numeric features

# Merging the text model and numeric input
merged = concatenate([dropout_layer, numeric_input])

# Adding Dense layers with L2 regularization
dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(merged)
dropout_dense_layer = Dropout(0.5)(dense_layer)

# Final output layer
output = Dense(1, activation='sigmoid')(dropout_dense_layer)

# Final model
final_model = Model(inputs=[text_input, numeric_input], outputs=output)

# Compile the model
final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary to inspect architecture
final_model.summary()

# Already added L2 regularization in Dense layer:
model_dense = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(merged)

model.add(Embedding(input_dim=5000, output_dim=256, input_length=50))

model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))  # Increased LSTM units

from keras.optimizers import Adam

optimizer = Adam(learning_rate=0.001)  # Try learning rates like 0.001 or 0.0001
final_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

from keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

final_model.fit([X_train_text, X_train_numeric], y_train, epochs=10, batch_size=64, validation_data=([X_test_text, X_test_numeric], y_test), callbacks=[early_stopping])

# Training the model with regularization and tuning
history = final_model.fit([X_train_text, X_train_numeric], y_train,
                          epochs=10,
                          batch_size=64,
                          validation_data=([X_test_text, X_test_numeric], y_test),
                          callbacks=[early_stopping])

# Evaluate the model
final_model.evaluate([X_test_text, X_test_numeric], y_test)

# Evaluate the model's performance on the test data
test_loss, test_accuracy = final_model.evaluate([X_test_text, X_test_numeric], y_test)

print(f"Test Accuracy: {test_accuracy}")
print(f"Test Loss: {test_loss}")

from sklearn.metrics import classification_report

# Predicting the labels for the test set
y_pred = (final_model.predict([X_test_text, X_test_numeric]) > 0.5).astype("int32")

# Print classification report
print(classification_report(y_test, y_pred))

# Evaluate model on test data
test_loss, test_accuracy = final_model.evaluate([X_test_text, X_test_numeric], y_test)

# Predict labels
y_pred = (final_model.predict([X_test_text, X_test_numeric]) > 0.5).astype("int32")

# Classification report
print(f"Test Accuracy: {test_accuracy}")
print(f"Test Loss: {test_loss}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Arrest', 'Arrest'], yticklabels=['No Arrest', 'Arrest'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve and AUC
y_pred_proba = final_model.predict([X_test_text, X_test_numeric])
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.title('Receiver Operating Characteristic')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.show()

